{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Crear entorno virtual\n",
    "python -m venv env\n",
    "\n",
    "# Activar entorno virtual\n",
    "source env/bin/activate\n",
    "\n",
    "# instalar paquetes\n",
    "pip install duckdb pyodbc pandas polars pyarrow pyspark arrow python-dotenv faker dotenv\n",
    "\n",
    "# instalar el cliente de DuckDB\n",
    "winget install DuckDB.cli\n",
    "\n",
    "# Guardar dependencias\n",
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos generados en 28.4734 segundos\n",
      "Datos insertados en 0.3889 segundos\n",
      "   registros\n",
      "0     500000\n",
      "Datos count(*) en 0.0263 segundos\n",
      "   id           poblacion       x       y\n",
      "0   0        Changchester  985772  476857\n",
      "1   1      West Tammyfort  305711  517204\n",
      "2   2            Hullport  435829  780857\n",
      "3   3       Howardborough  117952  223093\n",
      "4   4         West Donald  963395  812427\n",
      "5   5      New Laurenside  152315  198129\n",
      "6   6          West Corey  882371  836174\n",
      "7   7  Port Gabriellafort  359783  962460\n",
      "8   8    West Ryanborough  304137  106646\n",
      "9   9          Ramoshaven  122579  790135\n"
     ]
    }
   ],
   "source": [
    "# crear BBDD de poblaciones\n",
    "\n",
    "import time \n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "\n",
    "# Inicializar Faker para generar nombres aleatorios\n",
    "faker = Faker()\n",
    "Faker.seed(0)  # Fijar la semilla para reproducibilidad\n",
    "\n",
    "# Conectar a la base de datos DuckDB\n",
    "con = duckdb.connect('./env/data/bbdd_duckdb2.db', read_only=False)\n",
    "\n",
    "# Borrar la tabla si existe\n",
    "con.execute(\"DROP TABLE IF EXISTS poblaciones\")\n",
    "\n",
    "# Crear la tabla si no existe\n",
    "con.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS poblaciones (\n",
    "        id BIGINT PRIMARY KEY, \n",
    "        poblacion VARCHAR, \n",
    "        x INTEGER,\n",
    "        y INTEGER\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Generar datos aleatorios\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 500_000\n",
    "t = time.time()\n",
    "df = pd.DataFrame({\n",
    "    'id': np.arange(n),\n",
    "    'poblacion': [faker.city() for _ in range(n)],  # Generar nombres aleatorios\n",
    "    'x': np.random.randint(0, 1000000, n),  # N칰meros aleatorios para simular otro dato\n",
    "    'y': np.random.randint(0, 1000000, n)  # N칰meros aleatorios para simular otro dato\n",
    "})\n",
    "\n",
    "print(f\"Datos generados en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "# Registrar el DataFrame en DuckDB como una tabla temporal\n",
    "con.register('tmp_poblaciones', df)\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "# Insertar los datos de manera eficiente\n",
    "con.execute(\"INSERT INTO poblaciones SELECT * FROM tmp_poblaciones\")\n",
    "print (f\"Datos insertados en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "t = time.time()\n",
    "# Mostrar un resumen de la tabla\n",
    "print(con.execute(\"SELECT COUNT(*) AS registros FROM poblaciones\").fetchdf())\n",
    "print (f\"Datos count(*) en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "# Mostrar las 10 posiciones con la poblaci칩n m치s alta\n",
    "print(con.execute(\"SELECT * FROM poblaciones ORDER BY id asc LIMIT 10\").fetchdf())\n",
    "\n",
    "# Cerrar la conexi칩n\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos generados en 34.0981 segundos\n",
      "Datos insertados en duckdb_df: 0.1020 segundos\n",
      "Datos insertados en duck_db: 0.2411 segundos\n",
      "   registros\n",
      "0     500000\n"
     ]
    }
   ],
   "source": [
    "# formas mas elegantes de insertar datos: con panda\n",
    "\n",
    "import time\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "\n",
    "# Inicializar Faker para generar nombres aleatorios\n",
    "faker = Faker()\n",
    "Faker.seed(0)  # Fijar la semilla para reproducibilidad\n",
    "\n",
    "# Conectar a la base de datos DuckDB\n",
    "con = duckdb.connect('./env/data/bbdd_duckdb.db', read_only=False)\n",
    "\n",
    "np.random.seed(0)\n",
    "n = 500_000\n",
    "t = time.time()\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'id': np.arange(n),\n",
    "    'poblacion': [faker.city() for _ in range(n)],  # Generar nombres de ciudades\n",
    "    'x': np.random.randint(0, 1000000, n),\n",
    "    'y': np.random.randint(0, 1000000, n)\n",
    "})\n",
    "\n",
    "print(f\"Datos generados en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "t = time.time()\n",
    "# 游댳 Crear una tabla temporal en DuckDB desde el DataFrame\n",
    "duckdb_df = con.from_df(df)\n",
    "print (f\"Datos insertados en duckdb_df: {time.time() - t:.4f} segundos\")\n",
    "\n",
    "t = time.time()\n",
    "# 游댳 Crear la tabla real desde la tabla temporal\n",
    "con.execute(\"DROP TABLE IF EXISTS poblaciones\")\n",
    "con.execute(\"CREATE TABLE IF NOT EXISTS poblaciones AS SELECT * FROM duckdb_df\")\n",
    "print(f\"Datos insertados en duck_db: {time.time() - t:.4f} segundos\")\n",
    "\n",
    "### si la tabla ya existe\n",
    "## con.execute(\"INSERT INTO poblaciones SELECT * FROM duckdb_df\")\n",
    "\n",
    "# Verificar que los datos est치n en la tabla\n",
    "print(con.execute(\"SELECT COUNT(*) registros FROM poblaciones\").fetchdf())\n",
    "\n",
    "# Cerrar conexi칩n\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos generados en 33.9158 segundos\n",
      "Datos insertados en 0.1594 segundos\n",
      "   registros\n",
      "0     500000\n"
     ]
    }
   ],
   "source": [
    "# formas mas elegantes de insertar datos: con arrow\n",
    "\n",
    "import duckdb\n",
    "import pyarrow as pa\n",
    "import numpy as np\n",
    "import time\n",
    "from faker import Faker\n",
    "\n",
    "faker = Faker()\n",
    "Faker.seed(0)  # Fijar la semilla para reproducibilidad\n",
    "\n",
    "# Conectar a DuckDB\n",
    "con = duckdb.connect('./env/data/bbdd_duckdb.db')\n",
    "\n",
    "# Generar datos aleatorios en Apache Arrow\n",
    "np.random.seed(0)\n",
    "n = 500_000\n",
    "t = time.time()\n",
    "\n",
    "arrow_df = pa.table({\n",
    "    'id': pa.array(np.arange(n), type=pa.int64()),  # ID como BigInt\n",
    "    'poblacion': pa.array([faker.city() for _ in range(n)], type=pa.string()),  # Nombres aleatorios\n",
    "    'x': pa.array(np.random.randint(0, 1000000, n), type=pa.int32()),  # Coordenada X\n",
    "    'y': pa.array(np.random.randint(0, 1000000, n), type=pa.int32())   # Coordenada Y\n",
    "})\n",
    "\n",
    "print(f\"Datos generados en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "t = time.time()\n",
    "# 游댳 Insertar en DuckDB directamente desde Apache Arrow\n",
    "con.register(\"arrow_df\", arrow_df)\n",
    "con.execute(\"DROP TABLE IF EXISTS poblaciones\")\n",
    "con.execute(\"CREATE TABLE IF NOT EXISTS poblaciones AS SELECT * FROM arrow_df\")\n",
    "print(f\"Datos insertados en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "# Verificar que los datos est치n en la tabla\n",
    "print(con.execute(\"SELECT COUNT(*) registros FROM poblaciones\").fetchdf())\n",
    "\n",
    "# Cerrar conexi칩n\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   registros\n",
      "0     500000\n",
      "Datos count* en 0.0040 segundos\n",
      "   registros\n",
      "0      76180\n",
      "Datos GROUP BY en 0.0390 segundos\n",
      "   registros\n",
      "0         36\n",
      "Datos GROUP BY HAVING 250 en 0.0292 segundos\n",
      "            poblacion  registros\n",
      "0          Port James        299\n",
      "1       North Michael        404\n",
      "2          Smithmouth        258\n",
      "3         New Michael        404\n",
      "4        Lake Michael        403\n",
      "5       South Michael        395\n",
      "6        South Robert        257\n",
      "7        Michaelmouth        314\n",
      "8           New David        275\n",
      "9         South James        259\n",
      "10          New James        283\n",
      "11          West John        257\n",
      "12         Lake David        281\n",
      "13         East James        276\n",
      "14         West David        273\n",
      "15         East David        289\n",
      "16        North James        260\n",
      "17       West Michael        419\n",
      "18        West Robert        260\n",
      "19         West James        283\n",
      "20      Lake Jennifer        264\n",
      "21     South Jennifer        264\n",
      "22      East Jennifer        256\n",
      "23        South David        302\n",
      "24         Port David        291\n",
      "25      West Jennifer        288\n",
      "26       East Michael        407\n",
      "27   East Christopher        272\n",
      "28        North David        258\n",
      "29       Port Michael        407\n",
      "30     North Jennifer        280\n",
      "31           New John        278\n",
      "32  North Christopher        267\n",
      "33         Lake James        251\n",
      "34   Port Christopher        254\n",
      "35         New Robert        258\n",
      "Datos SELECT GROUP BY HAVING 250 en 0.0285 segundos\n",
      "   registros\n",
      "0          2\n",
      "Datos SEEK en 0.0020 segundos\n"
     ]
    }
   ],
   "source": [
    "# queries varias tipo sql\n",
    "\n",
    "import time \n",
    "import duckdb\n",
    "\n",
    "# Conectar a la base de datos DuckDB\n",
    "con = duckdb.connect('./env/data/bbdd_duckdb.db', read_only=False)\n",
    "\n",
    "t = time.time()\n",
    "# Mostrar un resumen de la tabla\n",
    "print(con.execute(\"SELECT COUNT(*) AS registros FROM poblaciones\").fetchdf())\n",
    "print (f\"Datos count* en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "t = time.time()\n",
    "# Mostrar un resumen de la tabla\n",
    "print(con.execute(\"SELECT COUNT(*) as registros FROM (SELECT poblacion, count(*) FROM poblaciones group by ALL) AS V\").fetchdf())\n",
    "print (f\"Datos GROUP BY en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "t = time.time()\n",
    "# Mostrar un resumen de la tabla\n",
    "print(con.execute(\"SELECT COUNT(*) as registros FROM (SELECT poblacion, count(*) FROM poblaciones group by ALL HAVING count(*) > 250) AS V\").fetchdf())\n",
    "print (f\"Datos GROUP BY HAVING 250 en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "t = time.time()\n",
    "# Mostrar un resumen de la tabla\n",
    "print(con.execute(\"SELECT poblacion, count(*) registros FROM poblaciones group by ALL HAVING count(*) > 250\").fetchdf())\n",
    "print (f\"Datos SELECT GROUP BY HAVING 250 en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "poblacion = faker.city()\n",
    "t = time.time()\n",
    "# Mostrar un resumen de la tabla\n",
    "print(con.execute(f\"SELECT COUNT(*) registros FROM poblaciones WHERE poblacion = '{poblacion}'\").fetchdf())\n",
    "print (f\"Datos SEEK en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "# Cerrar la conexi칩n\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PANDAS\n",
      "------\n",
      "Datos SELECT en 0.0800 segundos\n",
      "Datos exportados a csv en 0.6770 segundos\n",
      "Datos exportados a parquet en 0.1655 segundos\n",
      "\n",
      "POLARS\n",
      "------\n",
      "Datos SELECT en 0.0450 segundos\n",
      "Datos exportados a csv en 0.0495 segundos\n",
      "Datos exportados a parquet en 0.0740 segundos\n"
     ]
    }
   ],
   "source": [
    "# exportar resultado a csv, y parquet con pandas y polars\n",
    "\n",
    "import duckdb\n",
    "import time\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "# Conectar a la base de datos DuckDB\n",
    "con = duckdb.connect('./env/data/bbdd_duckdb.db', read_only=True)\n",
    "\n",
    "print (\"PANDAS\")\n",
    "print (\"------\")\n",
    "\n",
    "t = time.time()\n",
    "# dataframe\n",
    "df = con.execute(\"SELECT * FROM poblaciones\").fetchdf()\n",
    "print (f\"Datos SELECT en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "t = time.time()\n",
    "df.to_csv('./env/export/poblaciones.csv', index=False)\n",
    "print (f\"Datos exportados a csv en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "t = time.time()\n",
    "df.to_parquet('./env/export/poblaciones.parquet', index=False)\n",
    "print (f\"Datos exportados a parquet en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "print (f\"\\nPOLARS\")\n",
    "print (\"------\")\n",
    "\n",
    "t = time.time()\n",
    "df = pl.from_arrow(con.execute(\"SELECT * FROM poblaciones\").fetch_arrow_table())\n",
    "print (f\"Datos SELECT en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "t = time.time()\n",
    "df.write_csv('poblaciones.csv')\n",
    "print (f\"Datos exportados a csv en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "t = time.time()\n",
    "df.write_parquet('poblaciones.parquet')\n",
    "print (f\"Datos exportados a parquet en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "# Cerrar la conexi칩n\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "# uso del cli de duckDB\n",
    "\n",
    "duckdb \"./env/data/bbdd_duckdb.db\"\n",
    "\n",
    "-- metadata\n",
    "show tables;\n",
    "describe poblaciones;\n",
    "PRAGMA table_info('poblaciones');\n",
    "SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'poblaciones';\n",
    "\n",
    "-- exportar la tabla poblaciones a un archivo csv\n",
    "COPY poblaciones TO './env/export/poblaciones_cli.csv' (HEADER TRUE, DELIMITER ',');\n",
    "\n",
    "-- importar y exportar datos csv\n",
    "DROP TABLE IF EXISTS poblaciones2;\n",
    "CREATE TABLE poblaciones2 AS SELECT * FROM poblaciones where 1 = 0;\n",
    "COPY poblaciones2 FROM './env/export/poblaciones_cli.csv' (HEADER TRUE, DELIMITER ',');\n",
    "SELECT COUNT(*) registros from poblaciones2;\n",
    "\n",
    "-- Exportar la tabla poblaciones a un archivo Parquet\n",
    "COPY poblaciones TO './env/export/poblaciones_cli.parquet' (FORMAT 'parquet');\n",
    "\n",
    "-- importar y exportar parquet\n",
    "DROP TABLE IF EXISTS poblaciones3;\n",
    "CREATE TABLE poblaciones3 AS SELECT * FROM poblaciones WHERE 1 = 0;\n",
    "COPY poblaciones3 FROM './env/export/poblaciones_cli.parquet' (FORMAT 'parquet');\n",
    "SELECT COUNT(*) AS registros FROM poblaciones3;\n",
    "\n",
    ".exit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exportado ./env/export/iceberg_data//poblaciones.parquet en 0.0508 segundos\n",
      "Datos insertados en Iceberg en 0.1397 segundos\n",
      "   registros\n",
      "0     500000\n"
     ]
    }
   ],
   "source": [
    "# exportar a iceberg, y leer iceberg (parquet)\n",
    "\n",
    "import duckdb\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Conectar a DuckDB\n",
    "con = duckdb.connect('./env/bbdd_duckdb.db')\n",
    "\n",
    "# Habilitar Iceberg\n",
    "con.execute(\"INSTALL iceberg;\")\n",
    "con.execute(\"LOAD iceberg;\")\n",
    "\n",
    "# 游댳 Crear directorio de salida (si no existe)\n",
    "iceberg_path = \"./env/export/iceberg_data/\"\n",
    "\n",
    "# Asegurar que el directorio existe\n",
    "os.makedirs(iceberg_path, exist_ok=True)\n",
    "\n",
    "total_rows = con.execute(\"SELECT COUNT(*) FROM poblaciones\").fetchone()[0]\n",
    "batch_size = 100_000  # N칰mero de registros por fragmento\n",
    "\n",
    "\n",
    "t1 = time.time()\n",
    "output_file = f\"{iceberg_path}/poblaciones.parquet\"\n",
    "query = f\"\"\"\n",
    "    COPY (SELECT * FROM poblaciones) \n",
    "    TO '{output_file}' (FORMAT 'parquet', OVERWRITE);\n",
    "\"\"\"\n",
    "con.execute(query)\n",
    "print(f\"Exportado {output_file} en {time.time() - t1:.4f} segundos\")\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "# batches\n",
    "\n",
    "# t1 = time.time()\n",
    "# for offset in range(0, total_rows, batch_size):\n",
    "#     t2 = time.time()\n",
    "#     output_file = f\"{iceberg_path}/poblaciones_part_{offset}.parquet\"\n",
    "#     query = f\"\"\"\n",
    "#         COPY (SELECT * FROM poblaciones LIMIT {batch_size} OFFSET {offset}) \n",
    "#         TO '{output_file}' (FORMAT 'parquet', PARTITION_BY (id), OVERWRITE);\n",
    "#     \"\"\"\n",
    "#     con.execute(query)\n",
    "#     print(f\"Exportado {output_file} en {time.time() - t2:.4f} segundos\")\n",
    "\n",
    "# print(f\"Datos exportados a Iceberg en {iceberg_path}, {time.time() - t1:.4f} segundos\")\n",
    "\n",
    "# 游댳 Crear un cat치logo Iceberg con ubicaci칩n en disco\n",
    "\n",
    "# drop table if exists\n",
    "con.execute(\"DROP TABLE IF EXISTS poblaciones_ice\")\n",
    "\n",
    "t = time.time()\n",
    "con.execute(f\"\"\"\n",
    "CREATE TABLE poblaciones_ice AS \n",
    "SELECT * FROM READ_PARQUET('{iceberg_path}poblaciones.parquet');\n",
    "\"\"\")\n",
    "print(f\"Datos insertados en Iceberg en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "print(con.execute(\"SELECT COUNT(*) registros FROM poblaciones_ice\").fetchdf())\n",
    "\n",
    "# Cerrar conexi칩n\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- exportar a iceberg, y leer iceberg (parquet) tablas m치s grandes\n",
    "\n",
    "-- LENTO !!!!\n",
    "\n",
    "duckdb \"./env/data/tpch-sf30.db\"\n",
    "\n",
    "-- metadata\n",
    "show tables;\n",
    "describe orders;\n",
    "\n",
    "SELECT o_orderdate, count(*) registros FROM orders group by all;\n",
    "\n",
    "COPY (SELECT * FROM orders) \n",
    "TO './env/export/iceberg_big_data/' (FORMAT 'parquet', PARTITION_BY (o_orderdate), OVERWRITE);\n",
    "\n",
    "-- Exportar la tabla poblaciones a un archivo Parquet\n",
    "COPY (SELECT * FROM orders limit 500_000) \n",
    "TO './env/export/orders.parquet' (FORMAT 'parquet', OVERWRITE);\n",
    "\n",
    "COPY (SELECT * FROM lineitem) \n",
    "TO './env/export/lineitem.parquet' (FORMAT 'parquet', OVERWRITE);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exportado ./env/export/iceberg_big_data/ [1992-01-01 00:00:00] en 1.0945 segundos\n",
      "Exportado ./env/export/iceberg_big_data/ [1992-01-02 00:00:00] en 1.0427 segundos\n",
      "Exportado ./env/export/iceberg_big_data/ [1992-01-03 00:00:00] en 1.0710 segundos\n",
      "Exportado ./env/export/iceberg_big_data/ [1992-01-04 00:00:00] en 1.0895 segundos\n",
      "Exportado ./env/export/iceberg_big_data/ [1992-01-05 00:00:00] en 1.0610 segundos\n",
      "Exportado ./env/export/iceberg_big_data/ [1992-01-06 00:00:00] en 1.2108 segundos\n",
      "Exportado ./env/export/iceberg_big_data/ [1992-01-07 00:00:00] en 1.1505 segundos\n",
      "Exportado ./env/export/iceberg_big_data/ [1992-01-08 00:00:00] en 1.1429 segundos\n",
      "Exportado ./env/export/iceberg_big_data/ [1992-01-09 00:00:00] en 1.0945 segundos\n",
      "Exportado ./env/export/iceberg_big_data/ [1992-01-10 00:00:00] en 1.0009 segundos\n",
      "Exportado TODO a ./env/export/iceberg_big_data/ [1992-01-10 00:00:00] en 10.9629 segundos\n",
      "Datos insertados en Iceberg en 0.1454 segundos\n",
      "   registros\n",
      "0      18924\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Conectar a DuckDB\n",
    "con = duckdb.connect('./env/data/tpch-sf30.db')\n",
    "\n",
    "# Habilitar Iceberg\n",
    "con.execute(\"INSTALL iceberg;\")\n",
    "con.execute(\"LOAD iceberg;\")\n",
    "\n",
    "# 游댳 Crear directorio de salida (si no existe)\n",
    "iceberg_path = \"./env/export/iceberg_big_data/\"\n",
    "\n",
    "# Asegurar que el directorio existe\n",
    "os.makedirs(iceberg_path, exist_ok=True)\n",
    "\n",
    "# batches\n",
    "# recuperar el array las fechas distintas de orders\n",
    "fechas = con.execute(\"SELECT distinct(o_orderdate) fecha FROM orders order by fecha\").fetchdf()\n",
    "\n",
    "# filtrar para las 30 primeras fechas\n",
    "fechas = fechas.head(30)\n",
    "\n",
    "t = time.time()\n",
    "# para cada fecha, exportar a iceberg\n",
    "for fecha in fechas['fecha']:\n",
    "\n",
    "    t1 = time.time()\n",
    "    output_file = f\"{iceberg_path}\"\n",
    "    query = f\"\"\"\n",
    "        COPY (SELECT * FROM orders WHERE o_orderdate = '{fecha}') \n",
    "        TO '{output_file}' (FORMAT 'parquet', PARTITION_BY(o_orderdate), OVERWRITE);\n",
    "    \"\"\"\n",
    "    con.execute(query)\n",
    "    print(f\"Exportado {output_file} [{fecha}] en {time.time() - t1:.4f} segundos\")\n",
    "\n",
    "print(f\"Exportado TODO a {output_file} [{fecha}] en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "# drop table if exists\n",
    "con.execute(\"DROP TABLE IF EXISTS poblaciones_big_table_ice\")\n",
    "\n",
    "t = time.time()\n",
    "con.execute(f\"\"\"\n",
    "    CREATE TABLE poblaciones_big_table_ice AS \n",
    "    SELECT * FROM READ_PARQUET('{iceberg_path}/*/*.parquet');\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Datos insertados en Iceberg en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "print(con.execute(\"SELECT COUNT(*) registros FROM poblaciones_big_table_ice\").fetchdf())\n",
    "\n",
    "# Cerrar conexi칩n\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              cliente  anio  total_ventas\n",
      "0  Customer#000414892  1994  2.606656e+06\n",
      "1  Customer#001439371  1997  2.592001e+06\n",
      "2  Customer#000650719  1995  2.558529e+06\n",
      "3  Customer#001390222  1994  2.528584e+06\n",
      "4  Customer#000845770  1992  2.466209e+06\n",
      "5  Customer#000643666  1994  2.429590e+06\n",
      "6  Customer#000130426  1995  2.414659e+06\n",
      "7  Customer#000083635  1996  2.392658e+06\n",
      "8  Customer#001032646  1995  2.353206e+06\n",
      "9  Customer#000309214  1994  2.347204e+06\n",
      "Consulta 1: Ventas Totales por Cliente y A침o\n",
      "Datos SELECT en 16.9094 segundos\n",
      "                                        producto  cantidad_total\n",
      "0               khaki plum antique blush frosted          2143.0\n",
      "1            lavender maroon lime black cornsilk          2083.0\n",
      "2   goldenrod cornflower chartreuse antique lace          2012.0\n",
      "3        firebrick cornflower yellow violet lace          1985.0\n",
      "4                    steel cream pale dark azure          1968.0\n",
      "5  blanched yellow gainsboro cornflower metallic          1968.0\n",
      "6      honeydew purple chartreuse peach cornsilk          1958.0\n",
      "7              lemon coral lawn tomato burlywood          1950.0\n",
      "8             brown lemon firebrick slate forest          1936.0\n",
      "9                plum grey royal white firebrick          1921.0\n",
      "Consulta 2: Ranking de Productos m치s Vendidos\n",
      "Datos SELECT en 47.1747 segundos\n"
     ]
    }
   ],
   "source": [
    "# consultas tipo tpch\n",
    "\n",
    "import time\n",
    "import duckdb\n",
    "\n",
    "con = duckdb.connect('./env/data/tpch-sf10.db')\n",
    "\n",
    "# 游댳 Group By: Ventas por cliente y a침o\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    c.c_name AS cliente,\n",
    "    EXTRACT(YEAR FROM o.o_orderdate) AS anio,\n",
    "    SUM(l.l_extendedprice * (1 - l.l_discount)) AS total_ventas\n",
    "FROM customer c\n",
    "JOIN orders o ON c.c_custkey = o.o_custkey\n",
    "JOIN lineitem l ON o.o_orderkey = l.l_orderkey\n",
    "GROUP BY cliente, anio\n",
    "ORDER BY total_ventas DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "t = time.time()\n",
    "\n",
    "# Mostrar resultados\n",
    "print(con.execute(query).fetchdf())\n",
    "print (f\"\\nConsulta 1: Ventas Totales por Cliente y A침o\")\n",
    "print (f\"Datos SELECT en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "# 游댳 Group By: Ventas por cliente y a침o\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    p.p_name AS producto,\n",
    "    SUM(l.l_quantity) AS cantidad_total\n",
    "FROM part p\n",
    "JOIN lineitem l ON p.p_partkey = l.l_partkey\n",
    "GROUP BY producto\n",
    "ORDER BY cantidad_total DESC\n",
    "LIMIT 10;\n",
    "\"\"\"\n",
    "\n",
    "t = time.time()\n",
    "# Mostrar resultados\n",
    "print(con.execute(query).fetchdf())\n",
    "print (f\"\\nConsulta 2: Ranking de Productos m치s Vendidos\")\n",
    "print (f\"Datos SELECT en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Creating secret...\n",
      "Datos insertados en 24.6198 segundos\n",
      "   registros\n",
      "0    1943833\n",
      "Datos count(*) en 0.0017 segundos\n"
     ]
    }
   ],
   "source": [
    "# azure como fuente de datos\n",
    "\n",
    "import polars as pl\n",
    "import duckdb\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "con = duckdb.connect('./env/data/bbdd_duckdb.db')\n",
    "\n",
    "azure_storage_connection_string = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n",
    "\n",
    "    # DefaultEndpointsProtocol=https;\n",
    "    # AccountName={accountName};\n",
    "    # AccountKey={key};\n",
    "    # EndpointSuffix=core.windows.net\n",
    "\n",
    "container_name = os.getenv('CONTAINER_NAME')\n",
    "folder_name = os.getenv('FOLDER_NAME')\n",
    "blob_prefix = os.getenv('BLOB_PREFIX')\n",
    "blob_sufix = os.getenv('BLOB_SUFIX')\n",
    "\n",
    "path = f\"az://{container_name}/{folder_name}/{blob_prefix}{blob_sufix}\"\n",
    "\n",
    "# Install and load the Azure connector\n",
    "con.execute(\"INSTALL azure;\")\n",
    "con.execute(\"LOAD azure;\")\n",
    "\n",
    "print(f\" Creating secret...\")\n",
    "\n",
    "con.execute(f\"\"\"\n",
    "    CREATE OR REPLACE SECRET secret1 (\n",
    "    TYPE AZURE,\n",
    "    CONNECTION_STRING '{azure_storage_connection_string}'\n",
    ");\"\"\")\n",
    "\n",
    "t = time.time()\n",
    "con.execute(\"DROP TABLE IF EXISTS teams_logs;\")\n",
    "con.execute(f\"\"\"CREATE TABLE teams_logs AS SELECT * FROM '{path}';\"\"\")\n",
    "print (f\"Datos insertados en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "t = time.time()\n",
    "print(con.execute(\"SELECT COUNT(*) registros FROM teams_logs\").fetchdf())\n",
    "print (f\"Datos count(*) en {time.time() - t:.4f} segundos\")\n",
    "\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- queries actividades teams\n",
    "\n",
    "duckdb \"./env/data/bbdd_duckdb.db\"\n",
    "describe teams_logs;\n",
    "\n",
    "select min(datetime) min_date, max(datetime) max_date from teams_logs;\n",
    "\n",
    "SELECT department, count(*) registros FROM teams_logs\n",
    "WHERE datetime >= '2024-09-01' AND datetime < '2025-01-01'\n",
    "group by all;\n",
    "\n",
    "\n",
    "SELECT upper(jobTitle) , strftime(date_trunc('month', datetime), '%Y-%m') as date, count(*) registros FROM teams_logs\n",
    "WHERE datetime >= '2024-09-01' AND datetime < '2025-01-01'\n",
    "and department = 'DATA'\n",
    "group by all;\n",
    "\n",
    "\n",
    "PIVOT (\n",
    "    SELECT upper(jobTitle) jobTitle, strftime(datetime, '%Y-%m') as date, COUNT(*) registros FROM teams_logs\n",
    "    WHERE datetime >= '2024-09-01' AND datetime < '2025-01-01'\n",
    "    and department = 'DATA'\n",
    "    group by all\n",
    "    )\n",
    "ON date\n",
    "USING sum(registros);\n",
    "\n",
    "\n",
    "select * from teams_logs where displayName like '%Eladio%' limit 10;\n",
    "\n",
    "\n",
    "select activity, strftime(datetime, '%Y-%m') as date, strftime(datetime, '%HH') as hour, count(*) registros \n",
    "from teams_logs where displayName like '%Eladio%'\n",
    "GROUP BY all;\n",
    "\n",
    "PIVOT (\n",
    "    select activity, strftime(datetime, '%HH') as hour, count(*) registros \n",
    "    from teams_logs where displayName like '%Eladio%'\n",
    "    GROUP BY all\n",
    "    )\n",
    "ON hour\n",
    "USING sum(registros);\n",
    "\n",
    "PIVOT (\n",
    "    select activity, strftime(datetime, '%HH') as hour, count(*) registros \n",
    "    from teams_logs where department = 'DATA'\n",
    "    GROUP BY all\n",
    "    )\n",
    "ON hour\n",
    "USING sum(registros);\n",
    "\n",
    "\n",
    "\n",
    "WITH base_data AS (\n",
    "  SELECT \n",
    "    activity, \n",
    "    strftime(datetime, '%HH') as hour, \n",
    "    count(*) as registros \n",
    "  FROM teams_logs \n",
    "  WHERE department = 'DATA'\n",
    "  GROUP BY ALL\n",
    "),\n",
    "totals AS (\n",
    "  SELECT \n",
    "    activity, \n",
    "    SUM(registros) as total_registros\n",
    "  FROM base_data\n",
    "  GROUP BY activity\n",
    ")\n",
    "PIVOT (\n",
    "  SELECT \n",
    "    b.activity, \n",
    "    b.hour, \n",
    "    ROUND(b.registros * 100.0 / t.total_registros, 2) as percentage\n",
    "  FROM base_data b\n",
    "  JOIN totals t ON b.activity = t.activity\n",
    ")\n",
    "ON hour\n",
    "USING SUM(percentage);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "-- queries fabric\n",
    "\n",
    "duckdb \"./env/data/bbdd_duckdb.db\"\n",
    "\n",
    "INSTALL azure;\n",
    "LOAD azure;\n",
    "\n",
    "-- obtener token de acceso desde powershell\n",
    "Connect-AzAccount\n",
    "$testToken = Get-AzAccessToken -ResourceTypeName Storage\n",
    "# Retrieved token is of string type which you can validate with the \"$testToken.Token.GetTypeCode()\" command.\n",
    "$testToken.Token | Set-Clipboard\n",
    "\n",
    "-- crear secret\n",
    "CREATE OR REPLACE SECRET onelake (\n",
    "      TYPE AZURE,\n",
    "      PROVIDER ACCESS_TOKEN,\n",
    "      ACCESS_TOKEN 'secreto'\n",
    "  );\n",
    "\n",
    "FROM duckdb_secrets();\n",
    "show tables;\n",
    "\n",
    "\n",
    "CREATE VIEW onelake_orders AS SELECT * \n",
    "FROM delta_scan('abfss://<workspace_id>@onelake.dfs.fabric.microsoft.com/<database_id>/Tables/Orders');\n",
    "\n",
    "select count(*) registros from onelake_orders;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: C:\\Users\\erincon\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.4.tar.gz (317.3 MB)\n",
      "     ---------------------------------------- 0.0/317.3 MB ? eta -:--:--\n",
      "      -------------------------------------- 4.7/317.3 MB 28.4 MB/s eta 0:00:12\n",
      "     - ------------------------------------ 11.5/317.3 MB 31.3 MB/s eta 0:00:10\n",
      "     - ------------------------------------ 16.3/317.3 MB 27.6 MB/s eta 0:00:11\n",
      "     -- ----------------------------------- 23.6/317.3 MB 29.3 MB/s eta 0:00:11\n",
      "     --- ---------------------------------- 30.9/317.3 MB 30.7 MB/s eta 0:00:10\n",
      "     ---- --------------------------------- 39.1/317.3 MB 31.8 MB/s eta 0:00:09\n",
      "     ----- -------------------------------- 46.7/317.3 MB 32.3 MB/s eta 0:00:09\n",
      "     ------ ------------------------------- 54.5/317.3 MB 32.7 MB/s eta 0:00:09\n",
      "     ------- ------------------------------ 62.9/317.3 MB 33.4 MB/s eta 0:00:08\n",
      "     -------- ----------------------------- 69.7/317.3 MB 33.4 MB/s eta 0:00:08\n",
      "     --------- ---------------------------- 76.8/317.3 MB 33.5 MB/s eta 0:00:08\n",
      "     ---------- --------------------------- 83.9/317.3 MB 33.7 MB/s eta 0:00:07\n",
      "     ---------- --------------------------- 91.5/317.3 MB 33.7 MB/s eta 0:00:07\n",
      "     ----------- -------------------------- 97.8/317.3 MB 33.4 MB/s eta 0:00:07\n",
      "     ----------- ------------------------- 102.2/317.3 MB 33.0 MB/s eta 0:00:07\n",
      "     ----------- ------------------------- 102.2/317.3 MB 33.0 MB/s eta 0:00:07\n",
      "     ----------- ------------------------- 102.2/317.3 MB 33.0 MB/s eta 0:00:07\n",
      "     ----------- ------------------------- 102.2/317.3 MB 33.0 MB/s eta 0:00:07\n",
      "     ----------- ------------------------- 102.2/317.3 MB 33.0 MB/s eta 0:00:07\n",
      "     ----------- ------------------------- 102.2/317.3 MB 33.0 MB/s eta 0:00:07\n",
      "     ----------- ------------------------- 102.2/317.3 MB 33.0 MB/s eta 0:00:07\n",
      "     ----------- ------------------------- 102.2/317.3 MB 33.0 MB/s eta 0:00:07\n",
      "     ----------- ------------------------- 102.2/317.3 MB 33.0 MB/s eta 0:00:07\n",
      "     ----------- ------------------------- 102.2/317.3 MB 33.0 MB/s eta 0:00:07\n",
      "     ----------- ------------------------- 102.2/317.3 MB 33.0 MB/s eta 0:00:07\n",
      "     ----------- ------------------------- 102.2/317.3 MB 33.0 MB/s eta 0:00:07\n",
      "     ----------- ------------------------- 102.5/317.3 MB 18.4 MB/s eta 0:00:12\n",
      "     ------------ ------------------------ 103.8/317.3 MB 17.8 MB/s eta 0:00:12\n",
      "     ------------ ------------------------ 109.3/317.3 MB 18.0 MB/s eta 0:00:12\n",
      "     ------------- ----------------------- 117.2/317.3 MB 18.6 MB/s eta 0:00:11\n",
      "     -------------- ---------------------- 124.0/317.3 MB 19.1 MB/s eta 0:00:11\n",
      "     --------------- --------------------- 130.8/317.3 MB 19.5 MB/s eta 0:00:10\n",
      "     ---------------- -------------------- 138.1/317.3 MB 20.0 MB/s eta 0:00:09\n",
      "     ---------------- -------------------- 145.8/317.3 MB 20.4 MB/s eta 0:00:09\n",
      "     ----------------- ------------------- 153.6/317.3 MB 20.9 MB/s eta 0:00:08\n",
      "     ------------------ ------------------ 161.7/317.3 MB 21.4 MB/s eta 0:00:08\n",
      "     ------------------- ----------------- 169.3/317.3 MB 21.8 MB/s eta 0:00:07\n",
      "     -------------------- ---------------- 177.2/317.3 MB 22.2 MB/s eta 0:00:07\n",
      "     --------------------- --------------- 185.1/317.3 MB 22.6 MB/s eta 0:00:06\n",
      "     ---------------------- -------------- 191.1/317.3 MB 22.7 MB/s eta 0:00:06\n",
      "     ----------------------- ------------- 199.2/317.3 MB 23.1 MB/s eta 0:00:06\n",
      "     ------------------------ ------------ 207.1/317.3 MB 23.4 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 215.2/317.3 MB 23.8 MB/s eta 0:00:05\n",
      "     ------------------------- ----------- 222.0/317.3 MB 24.0 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 229.9/317.3 MB 24.3 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 238.0/317.3 MB 24.7 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 245.9/317.3 MB 24.9 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 253.0/317.3 MB 25.1 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 258.7/317.3 MB 25.1 MB/s eta 0:00:03\n",
      "     ------------------------------- ----- 266.9/317.3 MB 25.4 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 276.3/317.3 MB 25.5 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 285.7/317.3 MB 25.8 MB/s eta 0:00:02\n",
      "     ---------------------------------- -- 292.8/317.3 MB 25.9 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 300.7/317.3 MB 25.8 MB/s eta 0:00:01\n",
      "     ------------------------------------  309.1/317.3 MB 25.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  316.9/317.3 MB 25.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  317.2/317.3 MB 25.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  317.2/317.3 MB 25.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  317.2/317.3 MB 25.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  317.2/317.3 MB 25.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  317.2/317.3 MB 25.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  317.2/317.3 MB 25.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  317.2/317.3 MB 25.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  317.2/317.3 MB 25.9 MB/s eta 0:00:01\n",
      "     ------------------------------------  317.2/317.3 MB 25.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- 317.3/317.3 MB 21.6 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): still running...\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.4-py2.py3-none-any.whl size=317849808 sha256=db0e408379efd7ca21c3329bac54fe599b0836b9b183977bbf693fc744e02a1e\n",
      "  Stored in directory: c:\\users\\erincon\\appdata\\local\\pip\\cache\\wheels\\8d\\28\\22\\5dbae8a8714ef046cebd320d0ef7c92f5383903cf854c15c0c\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.4\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en desarrollo. copiar \n",
    "\n",
    "import duckdb\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Iniciar sesi칩n de Spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Conectar a DuckDB y leer los datos\n",
    "con = duckdb.connect('./env/data/bbdd_duckdb.db')\n",
    "\n",
    "con.execute(\"DROP TABLE IF EXISTS mi_tabla\")\n",
    "con.execute(\"CREATE TABLE mi_tabla (id INT, nombre STRING, fecha_creacion TIMESTAMP)\")\n",
    "con.execute(\"INSERT INTO mi_tabla VALUES (1, 'Ejemplo', now())\")\n",
    "\n",
    "# Obtener el DataFrame de DuckDB como Pandas\n",
    "df_pandas = con.query(\"SELECT * FROM mi_tabla\").to_df()\n",
    "\n",
    "# Convertir Pandas DataFrame a Spark DataFrame\n",
    "df_spark = spark.createDataFrame(df_pandas)\n",
    "\n",
    "# Guardar en OneLake (en formato Delta)\n",
    "df_spark.write.format(\"delta\").mode(\"append\").save(\"abfss://<workspace_id>@onelake.dfs.fabric.microsoft.com/<database_id>/Tables/NewOrders\")\n",
    "\n",
    "\n",
    "# Cerrar la conexi칩n\n",
    "con.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
